Index: Generating_data_tables/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\nimport numpy as np\nimport csv\nimport datatable as dt\n\nimport find_data as fd\nfrom Parameters import parameters as param\nfrom Generating_data_tables import generate_results as gr\nfrom Generating_data_tables import generate_controls as gc\nfrom Generating_data_tables import generate_trajectories as gt\n\n\ndef exclude_invalid_videos(trajectories, results_per_plate, bad_patches_folders, path):\n    # Remove the plates which have overlapping patches\n    folders_with_bad_patches = bad_patches_folders[bad_patches_folders[\"overlapping_patches\"] == True].reset_index(drop=True)\n    for i_folder in range(len(results_per_plate)):\n        if results_per_plate.loc[i_folder, \"folder\"] in list(folders_with_bad_patches[\"folder\"]):\n            results_per_plate.drop([i_folder], inplace=True)\n\n    # Remove plates which don't have enough video time, or have more than ~10-30 double frames (=> could be two worms)\n    #cleaned_results = results_per_plate[results_per_plate[\"total_video_time\"] >= 10000]\n    cleaned_results = results_per_plate[results_per_plate[\"total_tracked_time\"] >= 10000]\n    cleaned_results = cleaned_results[cleaned_results[\"avg_proportion_double_frames\"] <= 0.01]\n\n    print(\"Finished cleaning results. Cleaning trajectories...\")\n    # Once the bad folders have been excluded from clean_results, remove them from the trajectory file\n    valid_folders = pd.unique(cleaned_results[\"folder\"])  # pd.unique to keep order\n    # Initialize a dictionary with one key per column\n    cleaned_traj = {}\n    trajectory_columns = list(trajectories.columns)\n    for col in trajectory_columns:\n        cleaned_traj[col] = []\n    # Fill each dictionary list with the values in traj that correspond to valid folders\n    for i_line in range(len(trajectories)):\n        if i_line % 2000000 == 0:\n            print(\"Line \", i_line, \" / \", len(trajectories))\n        if trajectories[\"folder\"].iloc[i_line] in valid_folders:\n            for col in list(trajectories.columns):\n                cleaned_traj[col].append(trajectories[col].iloc[i_line])\n\n    print(\"Saving clean_trajectories...\")\n    columns = list(cleaned_traj.keys())\n    nb_of_lines = len(cleaned_traj[columns[0]])\n    csv_file = path + \"clean_trajectories.csv\"\n    try:\n        with open(csv_file, 'w') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=columns)\n            writer.writeheader()\n            for i in range(nb_of_lines):\n                if i % 2000000 == 0:\n                    print(\"Line \", i_line, \" / \", len(trajectories))\n                writer.writerow({col: cleaned_traj[col][i] for col in columns})\n    except IOError:\n        print(\"I/O error\")\n\n    cleaned_results.to_csv(path + \"clean_results.csv\")\n\n    #cleaned_traj[\"is_smoothed\"] = cleaned_traj[\"is_smoothed\"].astype(bool)\n    #for plate in valid_folders:\n    #\n    #    cleaned_traj = pd.concat([cleaned_traj, trajectories[trajectories[\"folder\"] == plate]])\n    #return cleaned_traj, cleaned_results\n\n\ndef generate_smooth_trajectories(path):\n    # Retrieve trajectories from the folder path and save them in one dataframe\n    if \"shortened\" in path:\n        is_shortened = True\n    else:\n        is_shortened = False\n    trajectories = fd.trajcsv_to_dataframe(fd.path_finding_traj(path), shortened=is_shortened)\n    print(\"Finished retrieving trajectories\")\n    # Smooth the trajectory\n    trajectories = gt.smooth_trajectory(trajectories, 2)\n    trajectories.to_csv(path + \"trajectories.csv\")\n\n\ndef generate_trajectories(path):\n    trajectories = pd.read_csv(path + \"trajectories.csv\")\n    # Add a column with the patch where the worm is (-1 is outside)\n    print(\"Computing where the worm is...\")\n    if \"model\" in path:\n        trajectories[\"patch_centroid\"], overlapping_patches = gt.in_patch_list(trajectories, using=\"centroid\")\n    else:\n        trajectories[\"patch_silhouette\"], overlapping_patches = gt.in_patch_list(trajectories, using=\"silhouette\")\n    overlapping_patches.to_csv(path + \"overlapping_patches.csv\")\n    print(\"Finished computing in which patch the worm is at each time step\")\n    print(\"Computing distances...\")\n    # Add a column with the distance the worm crawled since last time step, for each time step\n    # It should put 0 for the first point of every folder, but record distance even when there's a tracking hole\n    # Distances are computed now because they are used for average speed analysis in results_per_id.\n    # Doing speed analysis later is tiring because of the tracking holes that are not in the results_per_plate\n    # table anymore.\n    trajectories[\"distances\"] = gt.trajectory_distances(trajectories)\n    trajectories.to_csv(path + \"trajectories.csv\")\n    print(\"Finished computing distance covered by the worm at each time step\")\n\n\ndef generate_results_per_id(path):\n    print(\"Building results_per_id...\")\n    trajectories = pd.read_csv(path + \"trajectories.csv\")\n    print(\"Starting to build results_per_id from trajectories...\")\n    results_per_id = gr.make_results_per_id_table(trajectories)\n    print(\"Finished!\")\n    results_per_id.to_csv(path + \"results_per_id.csv\")\n    return 0\n\n\ndef generate_results_per_plate(path):\n    print(\"Aggregating and preprocessing results per plate...\")\n    print(\"Retrieving results...\")\n    trajectories = pd.read_csv(path + \"trajectories.csv\",\n                               index_col=0)  # index_col=0 is to prevent the addition of new index columns at each import\n    results_per_id = pd.read_csv(path + \"results_per_id.csv\", index_col=0)\n    print(\"Starting to build results_per_plate from results_per_id...\")\n    results_per_plate = gr.make_results_per_plate(results_per_id, trajectories)\n    results_per_plate.to_csv(path + \"results_per_plate.csv\")\n    return 0\n\n\ndef generate_clean_tables_and_speed(path):\n    print(\"Retrieving results and trajectories...\")\n    results_per_plate = pd.read_csv(path + \"results_per_plate.csv\", index_col=0)\n    trajectories = pd.read_csv(path + \"trajectories.csv\", index_col=0)\n    overlapping_patches = pd.read_csv(path + \"overlapping_patches.csv\")\n    print(\"Cleaning results...\")\n    exclude_invalid_videos(trajectories, results_per_plate, overlapping_patches, path)\n    print(\"Computing speeds...\")\n    # For faster execution, we compute speeds here (and not at the same time as distances), when invalid\n    # trajectories have been excluded\n    clean_trajectories = dt.fread(path + \"clean_trajectories.csv\")\n    clean_trajectories[\"speeds\"] = dt.Frame(gt.trajectory_speeds(clean_trajectories))\n    clean_trajectories.to_csv(path + \"clean_trajectories.csv\")\n    return 0\n\n\ndef generate_aggregated_visits(path, threshold_list):\n    print(\"Retrieving results and trajectories...\")\n    clean_results = pd.read_csv(path + \"clean_results.csv\", index_col=0).reset_index()\n    print(\"Adding aggregated visits info for thresholds \" + str(threshold_list) + \"...\")\n    new_results = gr.add_aggregate_visit_info_to_results(clean_results, threshold_list)\n    new_results.to_csv(path + \"clean_results.csv\", index=False)\n    print(\"Done!\")\n    return new_results  # return this because this function is also used dynamically\n\n\ndef generate(starting_from=\"\", test_pipeline=False, modeled_data=False, old_dataset=False, shorten_traj=False, force_linux=False, force_windows=False):\n    \"\"\"\n    Will generate the data tables starting more or less from scratch.\n    Argument = from which level to regenerate stuff.\n    Returns path.\n    test_pipeline: if set to True, will run in a subset of the path that has to be hardcoded in this function.\n    \"\"\"\n\n    # Data path\n    if (fd.is_linux() or force_linux) and not force_windows:  # Linux path\n        path = \"/media/admin/T7 Shield/Results_minipatches_retracked/\"\n        if test_pipeline:\n            path = \"/media/admin/T7 Shield/Results_minipatches_retracked_test/\"\n            if modeled_data:\n                path = \"/media/admin/T7 Shield/Results_minipatches_retracked_test_model_rw/\"\n        if shorten_traj:\n            path = \"/media/admin/T7 Shield/Results_minipatches_retracked_shortened/\"\n            if modeled_data:\n                path = \"/media/admin/T7 Shield/Results_minipatches_retracked_shortened_model_rw/\"\n                if test_pipeline:\n                    path = \"/media/admin/T7 Shield/Results_minipatches_retracked_test_shortened_model_rw/\"\n        if old_dataset:\n            path = \"/media/admin/Expansion/Only_Copy_Probably/Results_minipatches_20221108_clean_fp/\"\n            if test_pipeline:\n                path = \"/media/admin/Expansion/Only_Copy_Probably/Results_minipatches_20221108_clean_fp_less/\"\n                if modeled_data:\n                    path = \"/media/admin/Expansion/Only_Copy_Probably/Results_minipatches_20221108_clean_fp_less_model_rw/\"\n            elif modeled_data:\n                path = \"/media/admin/Expansion/Only_Copy_Probably/Results_minipatches_20221108_clean_fp_model_rw/\"\n    else:  # Windows path\n        path = \"E:/Results_minipatches_retracked/\"\n        if test_pipeline:\n            path = \"E:/Results_minipatches_retracked_test/\"\n        if modeled_data:\n            path = \"E:/Only_Copy_Probably/Results_minipatches_20221108_clean_fp_model_rw/\"\n        if shorten_traj:\n            path = \"E:/Results_minipatches_retracked_shortened/\"\n\n    if modeled_data:\n        print(\"Warning! To use modeled data, you must first generate it. \"\n              \"In order to do so, go to the script 'Scripts_model/..._random_walk_experimental_plates.py'!\")\n\n    if starting_from == \"controls\":\n        gc.generate_controls(path)\n        generate_smooth_trajectories(path)\n        generate_trajectories(path)\n        generate_results_per_id(path)\n        generate_results_per_plate(path)\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    if starting_from == \"smoothing\":\n        generate_smooth_trajectories(path)\n        generate_trajectories(path)\n        generate_results_per_id(path)\n        generate_results_per_plate(path)\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    if starting_from == \"trajectories\":\n        generate_trajectories(path)\n        generate_results_per_id(path)\n        generate_results_per_plate(path)\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    elif starting_from == \"results_per_id\":\n        generate_results_per_id(path)\n        generate_results_per_plate(path)\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    elif starting_from == \"results_per_plate\":\n        generate_results_per_plate(path)\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    elif starting_from == \"clean\":\n        generate_clean_tables_and_speed(path)\n        generate_aggregated_visits(path, param.threshold_list)\n\n    elif starting_from == \"only_beginning\":\n        gc.generate_controls(path)\n        generate_smooth_trajectories(path)\n        generate_trajectories(path)\n\n    return path\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Generating_data_tables/main.py b/Generating_data_tables/main.py
--- a/Generating_data_tables/main.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Generating_data_tables/main.py	(date 1729104110200)
@@ -19,8 +19,8 @@
 
     # Remove plates which don't have enough video time, or have more than ~10-30 double frames (=> could be two worms)
     #cleaned_results = results_per_plate[results_per_plate["total_video_time"] >= 10000]
-    cleaned_results = results_per_plate[results_per_plate["total_tracked_time"] >= 10000]
-    cleaned_results = cleaned_results[cleaned_results["avg_proportion_double_frames"] <= 0.01]
+    #cleaned_results = results_per_plate[results_per_plate["total_tracked_time"] >= 10000]
+    cleaned_results = results_per_plate[results_per_plate["avg_proportion_double_frames"] <= 0.01]
 
     print("Finished cleaning results. Cleaning trajectories...")
     # Once the bad folders have been excluded from clean_results, remove them from the trajectory file
Index: Parameters/parameters.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import copy\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mc\nimport colorsys\n\nfrom Parameters import patch_coordinates\n\n# General parameters\n\n# Just a parameter to toggle extensive printing in the functions (for debugging purposes)\nverbose = False\n\n# Time threshold list for visit aggregation\nthreshold_list = [100000]\n\n# Time threshold for leaving probability (to compute P_leave of a worm, look at probability that it leaves in the next\n#   N time steps, with N being this threshold)\ntime_threshold = 1\n\n# Number of pixel threshold to consider that a silhouette is invalid (if it has more than the threshold, invalid)\ninvalid_silhouette_threshold = 150\n\n# Time at which to cut video on the \"shortened\" path\ntime_to_cut_videos = 16000\n\n# Ratio for conversion between seconds and frames\n# (for now measured on just one plate)\none_frame_in_seconds = 0.82817\n\n# Condition names\nnb_to_name = {0: \"close 0.2\", 1: \"med 0.2\", 2: \"far 0.2\", 14: \"superfar 0.2\", 3: \"cluster 0.2\",\n              4: \"close 0.5\", 5: \"med 0.5\", 6: \"far 0.5\", 15: \"superfar 0.5\", 7: \"cluster 0.5\",\n              12: \"close 1.25\", 8: \"med 1.25\", 13: \"far 1.25\", 16: \"superfar 1.25\",\n              9: \"med 0.2+0.5\", 10: \"med 0.5+1.25\",\n              17: \"close 0\", 18: \"med 0\", 19: \"far 0\", 20: \"superfar 0\", 21: \"cluster 0\"}\nname_to_nb = {v: k for k, v in nb_to_name.items()}\n\nlist_by_distance = [17, 0, 4, 12, 18, 1, 5, 8, 19, 2, 6, 13, 20, 14, 15, 20, 16, 21, 3, 7]\nlist_by_density = [17, 18, 19, 20, 0, 1, 2, 14, 4, 5, 6, 15, 12, 8, 13, 16]\nlist_by_density_with_clusters = [17, 18, 19, 20, 21, 0, 1, 2, 14, 3, 4, 5, 6, 15, 7, 12, 8, 13, 16, 9, 10]\n\n# Distance to number of patch dictionary (lower we build a condition number to number of patches dictionary from that)\ndistance_to_nb_of_patches = {\"close\": 52, \"med\": 24, \"far\": 7, \"superfar\": 3, \"cluster\": 25}\n# nb_to_nb_of_patches = {0: 52, 1: 24, 2: 7, 3: 25, 4: 52, 5: 24, 6: 7, 7: 25, 8: 24, 9: 24, 10: 24, 11: 24}\n\n# Loops to make nice dictionaries from that:\n# nb_to_distance\n# nb_to_density\n# nb_to_nb_of_patches\n# name_to_nb_list\n\nnb_to_density = {}\nfor condition in nb_to_name.keys():\n    if \"0.2+0.5\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"0.2+0.5\"\n    elif \"0.5+1.25\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"0.5+1.25\"\n    # Put those after otherwise 0.5+1.25 would be classified as 0.5\n    elif \"0.2\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"0.2\"\n    elif \"0.5\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"0.5\"\n    elif \"1.25\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"1.25\"\n    # Put this after otherwise all conditions go to 0\n    elif \"0\" in nb_to_name[condition]:\n        nb_to_density[condition] = \"0\"\n    else:\n        nb_to_density[condition] = \"all\"\n\nnb_to_distance = {}\nfor condition in nb_to_name.keys():\n    if \"0\" in nb_to_density[condition]:\n        nb_to_distance[condition] = \"control\"\n    if \"close\" in nb_to_name[condition]:\n        nb_to_distance[condition] = \"close\"\n    elif \"med\" in nb_to_name[condition]:\n        nb_to_distance[condition] = \"med\"\n    elif \"superfar\" in nb_to_name[condition]:  # superfar before because \"far\" is in \"superfar\"\n        nb_to_distance[condition] = \"superfar\"\n    elif \"far\" in nb_to_name[condition]:\n        nb_to_distance[condition] = \"far\"\n    elif \"cluster\" in nb_to_name[condition]:\n        nb_to_distance[condition] = \"cluster\"\n\nnb_to_nb_of_patches = {}\nfor condition in nb_to_name.keys():\n    if \"close\" in nb_to_name[condition]:\n        nb_to_nb_of_patches[condition] = distance_to_nb_of_patches[\"close\"]\n    elif \"med\" in nb_to_name[condition]:\n        nb_to_nb_of_patches[condition] = distance_to_nb_of_patches[\"med\"]\n    elif \"superfar\" in nb_to_name[condition]:\n        nb_to_nb_of_patches[condition] = distance_to_nb_of_patches[\"superfar\"]\n    elif \"far\" in nb_to_name[condition]:\n        nb_to_nb_of_patches[condition] = distance_to_nb_of_patches[\"far\"]\n    elif \"cluster\" in nb_to_name[condition]:\n        nb_to_nb_of_patches[condition] = distance_to_nb_of_patches[\"cluster\"]\n\n# Convert condition names into lists of condition numbers\n# eg {\"close\": [0, 4], \"med\": [1, 5, 8, 9, 10, 11], \"far\": [2, 6]}\nname_to_nb_list = {\"all\": [], \"close\": [], \"med\": [], \"far\": [], \"superfar\": [], \"cluster\": [],\n                   \"0.5+1.25\": [], \"0.2+0.5\": [],\n                   \"0.2\": [], \"0.5\": [], \"1.25\": [],\n                   \"0\": []}\nfor condition in nb_to_name.keys():\n    name_to_nb_list[\"all\"].append(condition)  # the all should have everyone\n    name_to_nb_list[nb_to_name[condition]] = [\n        condition]  # \"condition\" to [condition_nb] conversion for single conditions\n    # For every pool defined in name_to_nb_list initialization, add all conditions that have this name in their name\n    # Note: first it finds distance, then will stop at first density found (which should be the right one)\n    # (this is to avoid all conditions being put in 0)\n    distance_found = False\n    for condition_pool in name_to_nb_list.keys():\n        if condition_pool in nb_to_name[condition] and (\n                distance_found or nb_to_name[condition][0] == condition_pool[0]):\n            name_to_nb_list[condition_pool].append(condition)\n            if not distance_found:\n                distance_found = True\n            else:\n                break  # order in name_to_nb list matters\n\n# Takes \"[12, 13, 14, 15]\" and returns \"0\"\n# Only works for lists that include all of the conditions\nnb_list_to_name = {str(v): k for k, v in name_to_nb_list.items()}\nnb_list_to_name = {str(sorted(v)): k for k, v in name_to_nb_list.items()}\nnb_list_to_name[\"[0, 4]\"] = \"close\"\nnb_list_to_name[\"[1, 5]\"] = \"med\"\nnb_list_to_name[\"[2, 6]\"] = \"far\"\nnb_list_to_name[\"[3, 7]\"] = \"cluster\"\nnb_list_to_name[\"[3, 7]\"] = \"cluster\"\nnb_list_to_name[\"[0, 4, 12]\"] = \"close\"\nnb_list_to_name[\"[1, 5, 13]\"] = \"med\"\nnb_list_to_name[\"[2, 6, 14]\"] = \"far\"\nnb_list_to_name[\"[3, 7, 15]\"] = \"cluster\"\nnb_list_to_name[\"[9, 10]\"] = \"mixed\"\nnb_list_to_name[\"[0, 1, 2, 14]\"] = \"0.2\"\nnb_list_to_name[\"[0, 1, 2, 14, 3]\"] = \"0.2\"\nnb_list_to_name[\"[4, 5, 6, 15]\"] = \"0.5\"\nnb_list_to_name[\"[4, 5, 6, 15, 7]\"] = \"0.5\"\nnb_list_to_name[\"[8, 12, 13, 16]\"] = \"1.25\"\nnb_list_to_name[\"[12, 8, 13, 16]\"] = \"1.25\"\n\n# Same but to list of names (eg \"close\" => [\"close 0\", \"close 0.2\", \"close 0.5\"]\nname_to_name_list = {\"all\": [], \"close\": [], \"med\": [], \"far\": [], \"superfar\": [], \"cluster\": [], \"0.5+1.25\": [],\n                     \"0.2+0.5\": [], \"0.5\": [], \"1.25\": [], \"0.2\": [], \"0\": []}\nfor name in name_to_name_list.keys():\n    condition_list = copy.deepcopy(name_to_nb_list[name])\n    for i_cond in range(len(condition_list)):\n        condition_list[i_cond] = nb_to_name[condition_list[i_cond]]  # convert each nb of nb_list to a name\n    name_to_name_list[name] = condition_list\n\n# Centers of patches for each condition\nnb_to_xy = {}\nfor condition in nb_to_distance.keys():\n    if nb_to_distance[condition] == \"close\":\n        nb_to_xy[condition] = patch_coordinates.xy_patches_close\n    if nb_to_distance[condition] == \"med\":\n        nb_to_xy[condition] = patch_coordinates.xy_patches_med\n    if nb_to_distance[condition] == \"far\":\n        nb_to_xy[condition] = patch_coordinates.xy_patches_far\n    if nb_to_distance[condition] == \"superfar\":\n        nb_to_xy[condition] = patch_coordinates.xy_patches_super_far\n    if nb_to_distance[condition] == \"cluster\":\n        nb_to_xy[condition] = patch_coordinates.xy_patches_cluster\n\n# Centers of patches for each condition\ndistance_to_xy = {\"close\": patch_coordinates.xy_patches_close, \"med\": patch_coordinates.xy_patches_med,\n                  \"far\": patch_coordinates.xy_patches_far, \"superfar\": patch_coordinates.xy_patches_super_far,\n                  \"cluster\": patch_coordinates.xy_patches_cluster}\n\n\n## Colors\n\n# close: purple\n# med: blue\n# far: green\n# superfar: yellowish green\n# clusters: teal\n# 0.2, 0.5, 1.25: shades of brown\n\ndef lighten_color(color, amount=0.5):\n    \"\"\"\n    Lightens the given color by multiplying (1-luminosity) by the given amount.\n    Input can be matplotlib color string, hex string, or RGB tuple.\n\n    Examples:\n    >> lighten_color('g', 0.3)\n    >> lighten_color('#F034A3', 0.6)\n    >> lighten_color((.3,.55,.1), 0.5)\n    \"\"\"\n    try:\n        c = mc.cnames[color]\n    except:\n        c = color\n    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n    r, g, b = colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n    return '#%02x%02x%02x' % (int(r * 255), int(g * 255), int(b * 255))\n\n\nname_to_color = {\"close\": \"mediumslateblue\", \"med\": \"royalblue\", \"far\": \"deepskyblue\", \"superfar\": \"turquoise\",\n                 \"cluster\": \"forestgreen\", \"0\": \"bisque\",\n                 \"0.2\": \"burlywood\", \"0.5\": \"darkgoldenrod\", \"1.25\": \"brown\", \"0.2+0.5\": \"chocolate\",\n                 \"0.5+1.25\": \"orange\",\n                 \"control\": \"gray\", \"all\": \"teal\",\n                 \"food\": \"brown\",\n                 \"mixed\": \"goldenrod\"}\n\n# Add colors for single conditions, distance override but color is lighter when density is 0.2, darker when 1.25\nfor condition in nb_to_name.keys():\n    distance_color = name_to_color[nb_to_distance[condition]]\n    density = nb_to_density[condition]\n    if \"+\" not in density:\n        density = float(density)\n    else:\n        density1, density2 = density.split(\"+\")\n        # for mixed densities, density will be intermediate\n        density = (float(density1) + float(density2)) / 2\n    name_to_color[nb_to_name[condition]] = lighten_color(mc.cnames[distance_color],\n                                                         amount=min(1.35, max(0.1, density) * 2))\n\n\n#name_to_color[\"med 1.25\"] = \"black\"\n#name_to_color[\"med 0.5+1.25\"] = \"grey\"\n\nname_to_rgb = {}\nfor name, color in name_to_color.items():\n    if type(name) == str:\n        name_to_rgb[name] = mc.to_rgb(color)\n    else:\n        name_to_rgb[name] = color\n\n\ndef test_colors():\n    y = 1\n    for pool_name, pool in name_to_nb_list.items():\n        x = 1\n        for cond in pool:\n            if len(pool) > 1:\n                plt.text(0, y, pool_name)\n                plt.scatter(x, y, color=name_to_color[nb_to_name[cond]], s=300)\n                x += 1\n        y += 1\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    test_colors()\n    import pandas as pd\n    name_to_rgb = pd.DataFrame(name_to_rgb)\n    name_to_rgb.to_csv(\"colors_mvt.csv\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Parameters/parameters.py b/Parameters/parameters.py
--- a/Parameters/parameters.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Parameters/parameters.py	(date 1729104789195)
@@ -27,6 +27,9 @@
 # (for now measured on just one plate)
 one_frame_in_seconds = 0.82817
 
+# Ratio for conversion between mm and pixels
+one_pixel_in_mm = 0
+
 # Condition names
 nb_to_name = {0: "close 0.2", 1: "med 0.2", 2: "far 0.2", 14: "superfar 0.2", 3: "cluster 0.2",
               4: "close 0.5", 5: "med 0.5", 6: "far 0.5", 15: "superfar 0.5", 7: "cluster 0.5",
Index: Scripts_sanity_checks/s20240927_interframe_times.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This is a script to plot the distribution of inter-frame times in our videos.\n# Expected results: a gaussian centered on 0.82 seconds\n# If that's not the case, and there are other peaks, then we're in the panade\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datatable as dt\nimport pandas as pd\nimport os\n\nimport analysis as ana\nfrom Generating_data_tables import main as gen\nfrom Scripts_analysis import s20240605_global_presence_heatmaps as heatmap_script\nfrom Scripts_models import s20240626_transitionmatrix as transition_script\nimport find_data as fd\nfrom Parameters import parameters as param\n\n\ndef plot_inter_frame_histogram():\n    path = gen.generate(\"\", shorten_traj=True)\n    results = dt.fread(path + \"clean_results.csv\")\n    trajectories = dt.fread(path + \"clean_trajectories.csv\")\n    full_list_of_folders = results[:, \"folder\"].to_list()[0]\n    print(\"Finished loading tables!\")\n    list_of_interframe_times = []\n\n    for i_folder, folder in enumerate(full_list_of_folders):\n        if i_folder % 10 == 0:\n            print(\"Folder \", i_folder, \" / \", len(full_list_of_folders))\n        current_traj = trajectories[dt.f.folder == folder, :]\n        list_of_frames = current_traj[:, \"frame\"].to_numpy()\n        list_of_times = current_traj[:, \"time\"].to_numpy()\n        _, are_times_invalid = fd.correct_time_stamps(current_traj.to_pandas(), False, True)\n        if not are_times_invalid:\n            frame_delta_each_line = list_of_frames[1:] - list_of_frames[:-1]\n            time_delta_each_line = list_of_times[1:] - list_of_times[:-1]\n            time_per_frame = ana.array_division_ignoring_zeros(time_delta_each_line, frame_delta_each_line)\n            list_of_interframe_times += list(np.ravel(time_per_frame))\n\n    plt.hist(list_of_interframe_times, bins=1000, log=True)\n    plt.show()\n\n\nplot_inter_frame_histogram()\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts_sanity_checks/s20240927_interframe_times.py b/Scripts_sanity_checks/s20240927_interframe_times.py
--- a/Scripts_sanity_checks/s20240927_interframe_times.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Scripts_sanity_checks/s20240927_interframe_times.py	(date 1729104788985)
@@ -38,6 +38,7 @@
             list_of_interframe_times += list(np.ravel(time_per_frame))
 
     plt.hist(list_of_interframe_times, bins=1000, log=True)
+    print("You can copy this number in Parameters/parameters.py, for the variable one_frame_in_seconds: ", np.mean(list_of_interframe_times))
     plt.show()
 
 
Index: Scripts_sanity_checks/s20240810_refpointsdistance.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># This is a script to plot the theoretical + recorded reference points,\n# scaled to all be on a 1847x1847 plate\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datatable as dt\nimport pandas as pd\nimport os\n\nimport analysis as ana\nfrom Generating_data_tables import main as gen\nfrom Scripts_analysis import s20240605_global_presence_heatmaps as heatmap_script\nimport find_data as fd\nfrom Parameters import parameters as param\n\npath = gen.generate(\"\")\nresults = dt.fread(path + \"clean_results.csv\")\ntrajectories = dt.fread(path + \"clean_trajectories.csv\")\nfull_list_of_folders = results[:, \"folder\"].to_list()[0]\nprint(\"Finished loading tables!\")\n\nall_conditions_list = param.nb_to_name.keys()\n\nif not os.path.isdir(path + \"perfect_heatmaps\"):\n    os.mkdir(path + \"perfect_heatmaps\")\n\ndistances_each_condition_top = []\ndistances_each_condition_left = []\ndistances_each_condition_right = []\ndistances_each_condition_bottom = []\n\nheatmap_points = np.zeros((1847, 1847))\n\ncondition_names = []\ncondition_colors = []\nfor i_condition, condition in enumerate(all_conditions_list):\n    if i_condition % 3 == 0:\n        print(\">>>>>> Condition \", i_condition, \" / \", len(all_conditions_list))\n    # Compute average radius from a few plates of this condition\n    plates_this_condition = fd.return_folders_condition_list(full_list_of_folders, condition)\n    for i_plate, plate in enumerate(plates_this_condition):\n        plate_metadata = fd.folder_to_metadata(plate)\n        xy_holes = plate_metadata[\"holes\"][0]\n        if len(xy_holes) == 4:\n            # Reorder points according to y then x, to get lower left corner then lower right then upper left then upper right\n            xy_holes = sorted(xy_holes, key=lambda x: x[1])\n            xy_holes = sorted(xy_holes[0:2], key=lambda x: x[0]) + sorted(xy_holes[2:4], key=lambda x: x[0])\n\n            point1, point4, point2, point3 = xy_holes\n            left_dist = ana.distance(point1, point2)\n            top_dist = ana.distance(point2, point3)\n            right_dist = ana.distance(point3, point4)\n            bottom_dist = ana.distance(point4, point1)\n\n            distances_each_condition_top.append(top_dist)\n            distances_each_condition_left.append(left_dist)\n            distances_each_condition_right.append(right_dist)\n            distances_each_condition_bottom.append(bottom_dist)\n        heatmap_points[int(point1[1]), int(point1[0])] += 1\n        heatmap_points[int(point2[1]), int(point2[0])] += 1\n        heatmap_points[int(point3[1]), int(point3[0])] += 1\n        heatmap_points[int(point4[1]), int(point4[0])] += 1\n\n    condition_names.append(param.nb_to_name[condition])\n    condition_colors.append(param.name_to_color[param.nb_to_name[condition]])\n\nfig, [ax0, ax1] = plt.subplots(1, 2)\n\n# Heatmap of the points\nax0.imshow(heatmap_points, cmap=\"hot\", vmax=0.1)\nax0.set_title(\"Heatmap of the reference points, vmax=0.1\")\n\n# Boxplot with one box for top edge, left edge, etc.\nax1.boxplot([distances_each_condition_left, distances_each_condition_top, distances_each_condition_right, distances_each_condition_bottom])\nax1.set_xticks([1, 2, 3, 4], [\"Left\", \"Top\", \"Right\", \"Bottom\"])\nax1.set_title(\"Reference point distance for each edge\")\n\nplt.show()\n\n# Heatmap of the points\nax0.imshow(heatmap_points, cmap=\"hot\", vmax=0.01)\nax0.set_title(\"Heatmap of the reference points, vmax=0.01\")\n\n# Boxplot with one box for top edge, left edge, etc.\nax1.boxplot([distances_each_condition_left, distances_each_condition_top, distances_each_condition_right, distances_each_condition_bottom])\nax1.set_xticks([1, 2, 3, 4], [\"Left\", \"Top\", \"Right\", \"Bottom\"])\nax1.set_title(\"Reference point distance for each edge\")\n\nplt.show()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts_sanity_checks/s20240810_refpointsdistance.py b/Scripts_sanity_checks/s20240810_refpointsdistance.py
--- a/Scripts_sanity_checks/s20240810_refpointsdistance.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Scripts_sanity_checks/s20240810_refpointsdistance.py	(date 1729104789371)
@@ -75,15 +75,8 @@
 ax1.set_xticks([1, 2, 3, 4], ["Left", "Top", "Right", "Bottom"])
 ax1.set_title("Reference point distance for each edge")
 
-plt.show()
-
-# Heatmap of the points
-ax0.imshow(heatmap_points, cmap="hot", vmax=0.01)
-ax0.set_title("Heatmap of the reference points, vmax=0.01")
-
-# Boxplot with one box for top edge, left edge, etc.
-ax1.boxplot([distances_each_condition_left, distances_each_condition_top, distances_each_condition_right, distances_each_condition_bottom])
-ax1.set_xticks([1, 2, 3, 4], ["Left", "Top", "Right", "Bottom"])
-ax1.set_title("Reference point distance for each edge")
+all_distances = distances_each_condition_top + distances_each_condition_bottom + distances_each_condition_left + distances_each_condition_right
+print("You can copy this number in Parameters/parameters.py, for the variable one_pixel_in_mm: ", np.mean(all_distances))
 
 plt.show()
+
Index: Scripts_sanity_checks/s20240810_n_eachcond.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Just a small script to print the N for each condition\nimport datatable as dt\nfrom Generating_data_tables import main as gen\nfrom Scripts_analysis import s20240605_global_presence_heatmaps as heatmap_script\nimport find_data as fd\nfrom Parameters import parameters as param\n\npath = gen.generate(\"\")\nresults = dt.fread(path + \"clean_results.csv\")\ntrajectories = dt.fread(path + \"clean_trajectories.csv\")\nfull_list_of_folders = results[:, \"folder\"].to_list()[0]\nprint(\"Finished loading tables!\")\n\nfor nb, name in param.nb_to_name.items():\n    nb_of_folders = len(fd.return_folders_condition_list(full_list_of_folders, nb))\n    print(name, \", n=\", nb_of_folders)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts_sanity_checks/s20240810_n_eachcond.py b/Scripts_sanity_checks/s20240810_n_eachcond.py
--- a/Scripts_sanity_checks/s20240810_n_eachcond.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Scripts_sanity_checks/s20240810_n_eachcond.py	(date 1729103717810)
@@ -1,13 +1,11 @@
 # Just a small script to print the N for each condition
 import datatable as dt
 from Generating_data_tables import main as gen
-from Scripts_analysis import s20240605_global_presence_heatmaps as heatmap_script
 import find_data as fd
 from Parameters import parameters as param
 
-path = gen.generate("")
+path = gen.generate("", shorten_traj=True)
 results = dt.fread(path + "clean_results.csv")
-trajectories = dt.fread(path + "clean_trajectories.csv")
 full_list_of_folders = results[:, "folder"].to_list()[0]
 print("Finished loading tables!")
 
Index: Scripts_analysis/s20240606_distancetoedgeanalysis.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># In this script, I will look at speed and visit time to pixels as a function of the distance to the edge of a patch\n\nfrom scipy import ndimage\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datatable as dt\n\nfrom Generating_data_tables import main as gen\nfrom Generating_data_tables import generate_results as gr\nfrom Parameters import parameters as param\nimport find_data as fd\nimport analysis as ana\nfrom Scripts_analysis import s20240605_global_presence_heatmaps as heatmap_script\n\n\ndef generate_patch_distance_map(in_patch_matrix, folder_to_save):\n    \"\"\"\n    Function that saves a map with for each pixel the distance to the closest food patch boundary, with positive values\n    outside food patches, and negative values inside (and boundary = 0).\n    @param in_patch_matrix: a numpy array containing for each pixel the patch to which it belongs (-1 for outside)\n    @param folder_to_save: the folder where the output map should be saved\n    @return: None\n    \"\"\"\n    zeros_inside = np.zeros(in_patch_matrix.shape)\n    zeros_outside = np.zeros(in_patch_matrix.shape)\n    for i in range(len(in_patch_matrix)):\n        for j in range(len(in_patch_matrix[i])):\n            if in_patch_matrix[i, j] == -1:\n                zeros_inside[i, j] = 1\n                zeros_outside[i, j] = 0\n            else:\n                zeros_inside[i, j] = 0\n                zeros_outside[i, j] = 1\n\n    # Create a distance matrix with 0 inside food patches and distance to boundary outside\n    distance_transform_outside = ndimage.distance_transform_edt(zeros_inside)\n    # Create a distance matrix with 0 outside food patches and distance to boundary inside\n    distance_transform_inside = ndimage.distance_transform_edt(zeros_outside)\n    # Subtract them from one another so that distance to patch boundary is positive outside, negative inside\n    # Add 1 to make inside (negative) distances go to zero at boundary\n    # And then remove 1 outside otherwise there'd be no 1 in the distances\n    distance_transform = distance_transform_outside - distance_transform_inside + 1 - zeros_inside\n\n    np.save(folder_to_save[:-len(folder_to_save.split(\"/\")[-1])] + \"distance_to_patch_map.npy\", distance_transform)\n\n\ndef pixel_visits_vs_distance_to_boundary(folder_list, traj, bin_list, variable=\"Total\"):\n    visit_values_each_bin_each_plate = np.zeros((len(bin_list), len(folder_list)))\n    visit_values_each_bin_each_plate[:] = np.nan\n    for i_folder, folder in enumerate(folder_list):\n        print(\">>> Folder \", i_folder, \" / \", len(folder_list))\n        # Correct the times for plates that have only NaNs or jumps in the time\n        current_traj = traj[dt.f.folder == folder, :]\n        corrected_times = fd.correct_time_stamps(current_traj.to_pandas(), True)[\"time\"]\n        current_traj[:, dt.f.time] = corrected_times\n\n        # Load the distance map\n        distance_map_path = folder[:-len(folder.split(\"/\")[-1])] + \"distance_to_patch_map.csv\"\n        if not os.path.isdir(distance_map_path):\n            in_patch_matrix_path = folder[:-len(\"traj.csv\")] + \"in_patch_matrix.csv\"\n            in_patch_matrix = pd.read_csv(in_patch_matrix_path).to_numpy()\n            generate_patch_distance_map(in_patch_matrix, folder)\n        distance_map = np.load(folder[:-len(folder.split(\"/\")[-1])] + \"distance_to_patch_map.npy\")\n        print(\">>>>>> Loaded distance map!\")\n\n        if variable == \"Total\":\n            pixel_wise_visits = heatmap_script.load_pixel_visits(current_traj[:, dt.f.time].to_list(),\n                                                                 folder)\n            print(\">>>>>> Loaded pixel_wise visit durations!\")\n            # Make it a list\n            current_folder_values = np.ravel(pixel_wise_visits)\n\n        if variable == \"Speed\":\n            pixel_wise_avg_speed = heatmap_script.load_avg_pixel_speed(current_traj, folder, regenerate=True)\n            print(\">>>>>> Loaded pixel_wise average speeds!\")\n            # Note: the pixels that were never visited are returned as NaN values by the load_avg_pixel_speed() function,\n            #       so they are not taken into account in the averaging.\n            current_folder_values = np.ravel(pixel_wise_avg_speed)\n\n        # In all cases, linearize the distance map to match pixel_wise values (one value per pixel)\n        current_folder_distances = np.ravel(distance_map)\n\n        # Put all the gathered values in bins corresponding to the bin_List argument\n        current_folder_distance_bins, current_folder_bin_values, [_, _], _ = ana.xy_to_bins(\n            list(current_folder_distances),\n            list(current_folder_values),\n            bin_size=None,\n            print_progress=False,\n            custom_bins=bin_list,\n            do_not_edit_xy=False,\n            compute_bootstrap=False)\n        for i_bin, distance_bin in enumerate(current_folder_distance_bins):\n            if distance_bin in bin_list:  # check this because ana.xy_to_bins() adds max value to the output bin list\n                bin_index = np.where(np.asarray(bin_list) == distance_bin)[0]\n                # Sometimes bin_index is a list with one element for some reason, if that's the case convert to int\n                if type(bin_index.astype(int)) != int and len(bin_index) > 0:\n                    bin_index = bin_index[0]\n                visit_values_each_bin_each_plate[bin_index][i_folder] = current_folder_bin_values[i_bin]\n    return visit_values_each_bin_each_plate\n\n\ndef plot_visit_duration_vs_distance(full_folder_list, traj, curve_names, bin_list, variable):\n    \"\"\"\n    Function that will make a plot with the duration of visits as a function of distance to the closest patch boundary\n    (negative distance => worm is inside the patch). Average is made over all pixels pooled together for each curve.\n    @param full_folder_list:\n    @param traj:\n    @param curve_names:\n    @param bin_list:\n    @param variable:\n    @return:\n    \"\"\"\n    tic = time.time()\n    curve_list = [param.name_to_nb_list[curve] for curve in curve_names]\n    for i_curve, curve in enumerate(curve_list):\n        print(int(time.time() - tic), \"s: Curve \", i_curve + 1, \" / \", len(curve_list))\n        folder_list = fd.return_folders_condition_list(full_folder_list, curve)\n        visit_values_each_bin_each_plate = pixel_visits_vs_distance_to_boundary(folder_list,\n                                                                                traj, bin_list,\n                                                                                variable=variable)\n\n        # At this point, visit_values has one value per bin/folder in each cell => average and bootstrap all that\n        nb_of_bins = len(bin_list)\n        avg_each_bin = np.empty(nb_of_bins)\n        avg_each_bin[:] = np.nan  # just a convenient way of having a table full of NaNs\n        # Errors\n        errors_inf = np.empty(nb_of_bins)\n        errors_sup = np.empty(nb_of_bins)\n        errors_inf[:] = np.nan\n        errors_sup[:] = np.nan\n        for i_bin in range(nb_of_bins):\n            # Rename\n            values_this_time_bin = visit_values_each_bin_each_plate[i_bin]\n            # and remove nan values for bootstrapping\n            values_this_time_bin = [values_this_time_bin[i] for i in range(len(values_this_time_bin)) if\n                                    not np.isnan(values_this_time_bin[i])]\n            if values_this_time_bin:\n                current_avg = np.nanmean(values_this_time_bin)\n                avg_each_bin[i_bin] = current_avg\n                bootstrap_ci = ana.bottestrop_ci(values_this_time_bin, 1000)\n                errors_inf[i_bin] = current_avg - bootstrap_ci[0]\n                errors_sup[i_bin] = bootstrap_ci[1] - current_avg\n\n        # Plot and add error bars\n        condition_name = \"OD = \"+param.nb_to_density[curve[0]]\n        if condition_name == \"OD = 0\":\n            condition_name = \"control\"\n        condition_color = param.name_to_color[param.nb_to_density[curve[0]]]\n        plt.plot(bin_list, avg_each_bin, color=condition_color, label=condition_name, linewidth=3)\n        plt.errorbar(bin_list, avg_each_bin, [errors_inf, errors_sup], fmt='.k', capsize=5)\n\n    print(\"Total time: \", int((time.time() - tic) // 60), \"min\")\n\n    if variable == \"Total\":\n        plt.title(\"Total time in pixel as a function of distance to the edge of the patch in \" + condition_name)\n        plt.ylabel(\"Total visit time to pixel\")\n    if variable == \"Speed\":\n        plt.title(\"Average centroid speed while in pixel as a function of distance to the edge of the patch in \" + condition_name)\n        plt.ylabel(\"Average centroid speed while in pixel\")\n    plt.xlabel(\"Distance to closest patch boundary ( <0 = inside)\")\n    plt.legend()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # Load path and clean_results.csv, because that's where the list of folders we work on is stored\n    path = gen.generate(shorten_traj=True)\n    results = pd.read_csv(path + \"clean_results.csv\")\n    trajectories = dt.fread(path + \"clean_trajectories.csv\")\n    full_list_of_folders = list(results[\"folder\"])\n    list_of_distance_bins = [-40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60]\n    plot_visit_duration_vs_distance(full_list_of_folders, trajectories,\n                                    ['med 0', 'med 0.2', 'med 0.5', 'med 1.25'], list_of_distance_bins,\n                                    variable=\"Speed\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Scripts_analysis/s20240606_distancetoedgeanalysis.py b/Scripts_analysis/s20240606_distancetoedgeanalysis.py
--- a/Scripts_analysis/s20240606_distancetoedgeanalysis.py	(revision 084c9e0350f4d81979e1d210889dcfd3bf1deced)
+++ b/Scripts_analysis/s20240606_distancetoedgeanalysis.py	(date 1729104177386)
@@ -150,24 +150,24 @@
             condition_name = "control"
         condition_color = param.name_to_color[param.nb_to_density[curve[0]]]
         plt.plot(bin_list, avg_each_bin, color=condition_color, label=condition_name, linewidth=3)
-        plt.errorbar(bin_list, avg_each_bin, [errors_inf, errors_sup], fmt='.k', capsize=5)
+        plt.errorbar(bin_list, avg_each_bin, [errors_inf, errors_sup], color=condition_color, capsize=5)
 
     print("Total time: ", int((time.time() - tic) // 60), "min")
 
     if variable == "Total":
-        plt.title("Total time in pixel as a function of distance to the edge of the patch in " + condition_name)
-        plt.ylabel("Total visit time to pixel")
+        plt.title("Total time in pixel as a function of distance to the edge of the patch in " + condition_name, fontsize=20)
+        plt.ylabel("Total visit time to pixel", fontsize=12)
     if variable == "Speed":
-        plt.title("Average centroid speed while in pixel as a function of distance to the edge of the patch in " + condition_name)
-        plt.ylabel("Average centroid speed while in pixel")
-    plt.xlabel("Distance to closest patch boundary ( <0 = inside)")
+        plt.title("Average centroid speed while in pixel as a function of distance to the edge of the patch in " + condition_name, fontsize=20)
+        plt.ylabel("Average centroid speed while in pixel", fontsize=12)
+    plt.xlabel("Distance to closest patch boundary ( <0 = inside)", fontsize=12)
     plt.legend()
     plt.show()
 
 
 if __name__ == "__main__":
     # Load path and clean_results.csv, because that's where the list of folders we work on is stored
-    path = gen.generate(shorten_traj=True)
+    path = gen.generate(starting_from="", shorten_traj=True)
     results = pd.read_csv(path + "clean_results.csv")
     trajectories = dt.fread(path + "clean_trajectories.csv")
     full_list_of_folders = list(results["folder"])
